<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/html" xmlns="http://www.w3.org/1999/html"
      xmlns="http://www.w3.org/1999/html">

<script type="text/javascript" charset="utf-8" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>



<head>
    <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/3.0.1/model-viewer.min.js"></script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="Truncated Consistency Models (TCM) for accelerated sampling in generative models">
    <meta name="author" content="GenAIR team">
    <link rel="icon" href="assets/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=IBM+Plex+Sans:300,400,500,700&display=swap" rel="stylesheet">

    <title>One-step Diffusion Models with \(f\)-Divergence Distribution Matching</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles -->
    <link rel="stylesheet" type="text/css" href="assets/style.css">
</head>

<body>
    <main role="main">
        <section class="jumbotron text-center" id="banner">
            <div class="container" id="title">
                <h2 class="jumbotron-heading">One-step Diffusion Models with \(f\)-Divergence Distribution Matching</h2>
            </div>

            <div class="container" style="max-width: 840px;">
                <div class="row row-author">
                    <div class="col-md author"><a href="https://yilun-xu.com/">Yilun Xu</a></div>
                    <div class="col-md author"> <a href="https://weilinie.github.io/">Weili Nie</a></div>
                    <div class="col-md author"><a href="http://latentspace.cc/">Arash Vahdat</a></div>
                </div>
                
                <div class="row" style="margin-top:0rem; margin-left: auto; margin-right: auto;">
                    <div class="col-md">
                        <div id="affiliation"><img width="20%" src="assets/nvidialogo.png"></div>
                    </div>
                </div>
            </div>


            <div class="buttons">
                <a href="https://drive.google.com/file/d/14cQSW9oTIpBXM0_VqxbN-bGbBDcxka5T/view?usp=sharing" target="_blank" class="btn btn-primary my-2">Paper</a>
                <a href="#" target="_blank" class="btn btn-primary my-2">Code (Coming Soon)</a>
            </div>

            <div class="container">
                <div class="row" id="pitch">
                    <p style="text-align: center;">
                       We present a general \(f\)-divergence minimization framework to distill multi-step diffusion models into a single step generator, with state-of-the-art one-step performance.
                    </p>
                </div>
            </div>
        </section>
    </main>
    <br>

    <center>
            <figure style="margin-top: 5px; margin-bottom: 10px;">
                <img width="50%" src="./assets/acceleration.png" style="margin-bottom: 15px;">
            </figure>
        </center>

        <center>
            <p style="margin-bottom: 5px;" class="caption">
                Inference latency of the teacher diffusion model and the \(f\)-distill. The FID scores are evaluated on ImageNet-64 (left) and MS-COCO (right)
            </p><p class="caption">
        </center>
        <center>
            <figure style="margin-top: 5px; margin-bottom: 5px;">
                <img width="50%" src="./assets/vis.png" style="margin-bottom: 5px;">
                <figcaption>Comparison between 50-step teacher (SD 1.5) and one-step student (\(f\)-distill) </figcaption>
            </figure>
        </center>

    <div class="container" style="padding-top: 30px;">
        <!-- Abstract -->
        <section id="abstract"/>
        <h5>Abstract</h5>
        <hr>
        <p style="text-align: justify;">
                Sampling from diffusion models involves a slow iterative process that hinders their practical deployment,
                especially for interactive applications.
                To accelerate generation speed, recent approaches distill a multi-step diffusion model into a single-step
                student generator via variational score distillation,
                which matches the distribution of samples generated by the student to the teacher's distribution.
                However, these approaches use the reverse Kullbackâ€“Leibler (KL)
                divergence for distribution matching which is known to be mode seeking. In this paper,
                <b>we generalize the distribution matching approach using a novel \(f\)-divergence
                minimization framework, termed \(f\)-distill, that covers different divergences with different trade-offs in terms of mode coverage and training variance.</b>
                We derive the gradient of the $f$-divergence between the teacher and student distributions and show that it is
                expressed as the product of their score differences and a weighting function determined by their density ratio.
                This weighting function naturally emphasizes samples with higher density in the teacher distribution,
                when using a less mode-seeking divergence. We observe that the popular variational score distillation approach using the
                reverse-KL divergence is a special case within our framework.
                Empirically, we demonstrate that alternative \(f\)-divergences, such as forward-KL
                and Jensen-Shannon divergences, outperform the current best variational score distillation
                methods across image generation tasks. In particular, <b>when using Jensen-Shannon divergence, \(f\)-distill
                achieves current state-of-the-art
                one-step generation performance on ImageNet64 and zero-shot text-to-image generation on MS-COCO</b>.
        </p>
        </section>

        <section id="method"/>
        <h5>General \(f\)-divergence Minimization</h5>
                <hr>
        <div class="flex-row">

            <center>
            <figure>
            <img width="70%" src="./assets/teaser.png" style="margin-bottom: 0px;">
            </figure>
            </center>

           <p style="margin-bottom: 5px;" class="caption">
                The gradient update for the one-step student in \(f\)-distill.
               The gradient is a product of the difference between the teacher score and fake score,
               and a weighting function determined by the chosen \(f\)-divergence and density ratio.
               The density ratio is readily available from the discriminator in the auxiliary GAN objective.
           </p><p class="caption">

            <p>
            </p>
            <p>
            </p>
        </div>
        </section>

        <section id="behavior"/>
        <h5>Behavior of different  \(f\)-divergences</h5>
        <hr>

        <center>
            <figure style="margin-top: 5px; margin-bottom: 5px;">
                <img width="90%" src="./assets/compare-f.png" style="margin-bottom: 5px;">
                <figcaption>Comparison of different \(f\)-divergences as a function of the likelihood ratio \(r :=p(\mathbf{x})/q(\mathbf{x})\) </figcaption>
            </figure>
        </center>
        </section>

        <section id="fid"/>
        <h5>Experimental Results</h5>
        <hr>

                <div class="flex-row">
            <p><b>Image Synthesis</b>: We extensively validate \(f\)-distill on class-conditioned ImageNet (\(64 \times 64\) and text-to-image tasks (512ps).
                In practice, we use <a href="https://arxiv.org/abs/2206.00364">EDM</a> as teacher model on ImageNet-64, and <a href="https://arxiv.org/abs/2112.10752">Stable Diffusion v1.5</a> as teacher model on text-to-image tasks.
                We demonstrate that \(f\)-distill achieves the new state-of-the-art single-step performance on class-conditioned ImageNet-64 and zero-shot text-to-image generation on MS-COCO,
                <b>obtaining the FID score of 1.16 on ImageNet-64, and the FID score 7.42 on zero-shot MS-COCO.</b>
                For reference, the FID scores of teachers are 2.35 and 8.59 on the two datasets, respectively.
            </p>

            <center>
            <figure style="margin-top: 5px; margin-bottom: 5px;">
                <img width="100%" src="./assets/combine_exp.png" style="margin-bottom: 5px;">
            </figure>
            </center>

            <p style="margin-bottom: 5px;" class="caption">
               Quantitative comparison of \(f\)-distill with state-of-the-art
                multi-step and one-step generation methods, on ImageNet-64 (right) and MS-COCO (left)
            </p><p class="caption">
        </section>

    <section id="paper">
        <h5>Paper</h5>
        <hr>
        <div class="flex-row">
            <div class="download-thumb">
            <div style="box-sizing: border-box; padding: 10px; margin: auto;">
                <a href="https://drive.google.com/file/d/14cQSW9oTIpBXM0_VqxbN-bGbBDcxka5T/view?usp=sharing"><img class="screenshot" src="assets/f-distill-preview.png"></a>
            </div>
        </div>
            <div class="paper-stuff">
                <p><b>One-step Diffusion Models with \(f\)-Divergence Distribution Matching</b></p>
                <p>Yilun Xu, Weili Nie, Arash Vahdat</p>
                <p><i></i></p>
                <div><span class="material-icons"> description </span><a href="https://drive.google.com/file/d/14cQSW9oTIpBXM0_VqxbN-bGbBDcxka5T/view?usp=sharing"> Paper</a></div>
                <div><span class="material-icons"> integration_instructions </span><a href=""> Code (coming soon)</a></div>
            </div>
            </div>
        </div>
    </section>

    <section id="bibtex">
    <h5>Citation</h5>
    <hr>

    <pre><code>@inproceedings{xu2024onestep,
    title={One-step Diffusion Models with f-Divergence Distribution Matching},
    author={Xu, Yilun and Nie, Weili and Vahdat, Arash},
    booktitle={},
    year={2024}
    }</code></pre>

    </section>
</div>

    <footer class="text-muted">
        <div class="container">
            <p class="float-right">
                <a href="#">Back to top</a>
            </p>
        </div>
    </footer>

</body>
</html>

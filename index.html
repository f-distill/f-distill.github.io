<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/html" xmlns="http://www.w3.org/1999/html"
      xmlns="http://www.w3.org/1999/html">

<script type="text/javascript" charset="utf-8" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>



<head>
    <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/3.0.1/model-viewer.min.js"></script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="One-step Diffusion Models with \(f\)-Divergence Distribution Matching">
    <meta name="author" content="GenAIR team">
    <link rel="icon" href="assets/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=IBM+Plex+Sans:300,400,500,700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
    <title>One-step Diffusion Models with \(f\)-Divergence Distribution Matching</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles -->
    <link rel="stylesheet" type="text/css" href="assets/style.css">
</head>

<body>
    <main role="main">
        <section class="jumbotron text-center" id="banner">
            <div class="container" id="title">
                <h2 class="jumbotron-heading">One-step Diffusion Models with \(f\)-Divergence Distribution Matching</h2>
            </div>

            <div class="container" style="max-width: 840px;">
                <div class="row row-author">
                    <div class="col-md author"><a href="https://yilun-xu.com/">Yilun Xu</a></div>
                    <div class="col-md author"> <a href="https://weilinie.github.io/">Weili Nie</a></div>
                    <div class="col-md author"><a href="http://latentspace.cc/">Arash Vahdat</a></div>
                </div>
               <br>
                <div class="row" style="margin-top:0rem; margin-left: auto; margin-right: auto;">
                    <div class="col-md">
                        <div id="affiliation"><img width="20%" src="assets/nvidialogo.png"></div>
                    </div>
                </div>
            </div>

            <br>

            <div class="buttons">
                <a href="https://drive.google.com/file/d/14cQSW9oTIpBXM0_VqxbN-bGbBDcxka5T/view?usp=sharing" target="_blank" class="btn btn-primary my-2">Paper</a>
                <a href="#" target="_blank" class="btn btn-primary my-2">Code (Coming Soon)</a>
            </div>

            <div class="container">
                <div class="row" id="pitch">
                    <p style="text-align: center;">
                       We present a general \(f\)-divergence minimization framework to distill multi-step diffusion models into a single step generator, with state-of-the-art one-step performance.
                    </p>
                </div>
            </div>
        </section>
    </main>
    <br>

    <div class="container" style="padding-top: 30px;">
        <center>
            <figure style="margin-top: 5px; margin-bottom: 10px;">
                <img width="90%" src="./assets/acceleration.png" style="margin-bottom: 15px;">
            </figure>
        </center>

        <center>
            <p style="margin-bottom: 5px;" class="caption">
                Inference latency of the teacher diffusion model and the \(f\)-distill. The FID scores are evaluated on ImageNet-64 (left) and MS-COCO (right).
                Notably, \(f\)-distill also outperforms the teacher model in terms of FID score.
            </p><p class="caption">
        </center>

        <br>

        <center>
            <figure style="margin-top: 5px; margin-bottom: 5px;">
                <img width="90%" src="./assets/vis.png" style="margin-bottom: 5px;">
            </figure>
        </center>

        <center>
            <p style="margin-bottom: 5px;" class="caption">
               Comparison between 50-step teacher (SD 1.5) and one-step student (\(f\)-distill)
            </p><p class="caption">
        </center>

        <br>

        <section id="abstract"/>
        <h5>Abstract</h5>
        <hr>
        <p style="text-align: justify;">
                Sampling from diffusion models involves a slow iterative process that hinders their practical deployment,
                especially for interactive applications.
                To accelerate generation speed, recent approaches distill a multi-step diffusion model into a single-step
                student generator via variational score distillation,
                which matches the distribution of samples generated by the student to the teacher's distribution.
                However, these approaches use the reverse Kullbackâ€“Leibler (KL)
                divergence for distribution matching which is known to be mode seeking. In this paper,
                <b>we generalize the distribution matching approach using a novel \(f\)-divergence
                minimization framework, termed \(f\)-distill, that covers different divergences with different trade-offs in terms of mode coverage and training variance.</b>
                We derive the gradient of the \(f\)-divergence between the teacher and student distributions and show that it is
                expressed as the product of their score differences and a weighting function determined by their density ratio.
                This weighting function naturally emphasizes samples with higher density in the teacher distribution,
                when using a less mode-seeking divergence. We observe that the popular variational score distillation approach using the
                reverse-KL divergence is a special case within our framework.
                Empirically, we demonstrate that alternative \(f\)-divergences, such as forward-KL
                and Jensen-Shannon divergences, outperform the current best variational score distillation
                methods across image generation tasks. In particular, <b>when using Jensen-Shannon divergence, \(f\)-distill
                achieves current state-of-the-art
                one-step generation performance on ImageNet64 and zero-shot text-to-image generation on MS-COCO</b>.
        </p>
        </section>

        <section id="method"/>
        <h5>General \(f\)-divergence Minimization</h5>
                <hr>

        <p>
            To distill a one-step student generator \(G_\theta\) from the multi-step teacher diffusion models,
            we aim to match the student distribution,
            denoted by \(q\),  with the teacher distribution \(p\).
            We perform this distribution matching by minimizing the \(f\)-divergence
            between the teacher and student distributions:
        </p>
            \[D_f(p||q) = \int p(\mathbf{x}) f\left(\frac{p(\mathbf{x})}{q(\mathbf{x})}\right) d\mathbf{x}\]

        <p>
            where \(f\) is a convex function that satisfies \(f(1) = 0\).
            Since the student distribution \(q\) is the push-forward measure induced by the one-step generator \(G_\theta\),
            it implicitly depends on the generator's parameters \(\theta\).
            Due to this implicit dependency, directly calculating the gradient of
            \(f\)-divergence, \(D_f(p||q)\), w.r.t \(\theta\) presents a challenge.
            In our paper, we establish the analytical expression for the gradient for
           \(D_f(p_t||q_t), \forall t \ge 0 \),  where \(p_t\) is the perturbed distribution through the
            diffusion forward process for the teacher's distribution \(p\),
            <i>i.e.,</i> \(p_t = p * \mathcal{N}( \mathbf{0},\sigma^2(t)\mathbf{I})\) (same for the student distribution \(q\)):
        </p>
            \[\nabla_\theta D_f(p_t||q_t) = \mathbb{E}_{\substack{\mathbf{z},  \epsilon}}-\left[f''\left(\frac{p_t(\mathbf{x})}{q_t(\mathbf{x})}\right)\left(\frac{p_t(\mathbf{x})}{q_t(\mathbf{x})}\right)^2 \left(\underbrace{\nabla_\mathbf{x} \log p_t(\mathbf{x})}_{\textrm{teacher score}} - \underbrace{\nabla_\mathbf{x} \log q_t(\mathbf{x})}_{\textrm{fake score}} \right)  \nabla_\theta G_\theta(\mathbf{z})\right]\]
        <p>
            where \(\mathbf{z} \sim p(\mathbf{z}), \epsilon \sim \mathcal{N}(  \mathbf{0}, \mathbf{I})\) and \( \mathbf{x} = G_\theta(\mathbf{z})+\sigma(t)\epsilon \). This gradient is expressed as the score difference
            between the teacher's and student's distributions, weighted 
            by a time-dependent factor 
            \(f''\left({p_t(\mathbf{x}_t)}/{q_t(\mathbf{x}_t)}\right)\left({p_t(\mathbf{x}_t)}/{q_t(\mathbf{x}_t)}\right)^2\)
            determined by both the 
            chosen \(f\)-divergence and the density ratio. Crucially, <b>every term in the gradient is tractable</b>,
            enabling the optimization of distributional matching through general \(f\)-divergence minimization. 
            In practice, the score of student distribution \(\nabla_\mathbf{x} \log q_{t}(\mathbf{x}_t)\)
            is approximated by an online diffusion model (fake score),
            and the density ratio \({p_t(\mathbf{x}_t)}/{q_t(\mathbf{x}_t)}\) in the weighting function
            is readily available from the discriminator in the auxiliary GAN objective.
            We provide illustration of the \(f\)-distill algorithm in the following figure.
        </p>

            <center>
            <figure>
            <img width="85%" src="./assets/teaser.png" style="margin-bottom: 0px;">
            </figure>
            </center>

           <p style="margin-bottom: 5px;" class="caption">
                The gradient update for the one-step student in \(f\)-distill.
               The gradient is a product of the difference between the teacher score and fake score,
               and a weighting function determined by the chosen \(f\)-divergence and density ratio.
               The density ratio is readily available from the discriminator in the auxiliary GAN objective.
           </p><p class="caption">

            <p>
            </p>
            <p>
            </p>
        </section>

        <section id="behavior"/>
        <h5>Behavior of different  \(f\)-divergences</h5>
        <hr>

        <center>
            <figure style="margin-top: 5px; margin-bottom: 5px;">
                <img width="90%" src="./assets/compare-f.png" style="margin-bottom: 5px;">
            </figure>
        </center>

        <p style="margin-bottom: 5px;" class="caption">
          Comparison of different \(f\)-divergences as a function of the likelihood ratio \(r :=p(\mathbf{x})/q(\mathbf{x})\)
        </p><p class="caption">
        </section>

        <section id="fid"/>
        <h5>Experimental Results</h5>
        <hr>

            <p> We extensively validate \(f\)-distill on class-conditioned ImageNet (\(64 \times 64\) and text-to-image tasks (512ps).
                In practice, we use <a href="https://arxiv.org/abs/2206.00364">EDM</a> as teacher model on ImageNet-64, and <a href="https://arxiv.org/abs/2112.10752">Stable Diffusion v1.5</a> as teacher model on text-to-image tasks.
                We demonstrate that \(f\)-distill achieves the new state-of-the-art single-step performance on class-conditioned ImageNet-64 and zero-shot text-to-image generation on MS-COCO,
                <b>obtaining the FID score of 1.16 on ImageNet-64, and the FID score 7.42 on zero-shot MS-COCO.</b>
                For reference, the FID scores of teachers are 2.35 and 8.59 on the two datasets, respectively.
            </p>

            <center>
            <figure style="margin-top: 5px; margin-bottom: 5px;">
                <img width="95%" src="./assets/combine_exp.png" style="margin-bottom: 5px;">
            </figure>
            </center>

            <p style="margin-bottom: 5px;" class="caption">
               Quantitative comparison of \(f\)-distill with state-of-the-art
                multi-step and one-step generation methods, on ImageNet-64 (right) and MS-COCO (left)
            </p><p class="caption">
        </section>

    <section id="paper">
        <h5>Paper</h5>
        <hr>
        <div class="flex-row">
            <div class="download-thumb">
            <a href="https://drive.google.com/file/d/14cQSW9oTIpBXM0_VqxbN-bGbBDcxka5T/view?usp=sharing"><img width="50%" class="screenshot" src="assets/f-distill-preview.png"></a>
            </div>

            <div class="paper-stuff">
                <p><b>One-step Diffusion Models with \(f\)-Divergence Distribution Matching</b></p>
                <p>Yilun Xu, Weili Nie, Arash Vahdat</p>
                <p><i></i></p>
                <div><span class="material-icons"> description </span><a href="https://drive.google.com/file/d/14cQSW9oTIpBXM0_VqxbN-bGbBDcxka5T/view?usp=sharing"> Paper</a></div>
                <div><span class="material-icons"> integration_instructions </span><a href=""> Code (coming soon)</a></div>
            </div>
        </div>
    </section>

    <h5>Citation</h5>
    <hr>
    <pre><code>@inproceedings{xu2024onestep,
    title={One-step Diffusion Models with f-Divergence Distribution Matching},
    author={Xu, Yilun and Nie, Weili and Vahdat, Arash},
    booktitle={},
    year={2024}}</code></pre>

    </div>

    <footer class="text-muted">
        <div class="container">
            <p class="float-right">
                <a href="#">Back to top</a>
            </p>
        </div>
    </footer>

</body>
</html>

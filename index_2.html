<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script type="text/javascript" charset="utf-8" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
}

@media screen and (min-width: 980px){
    body {
        width: 980px;
    }
}

h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #2677b5;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1, h2 {
    text-align: center;
}
h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
h3 {
    display: inline;
}
.paper-title {
    color: white;
    padding: 16px 0px 16px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.col-1 {
     width: 100%;
     float: left;
}

.author-row, .affil-row {
    font-size: 26px;
}

.author-row-new {
    color: white;
    text-align: center;
}

.author-row-new a {
    color: white;
    display: inline-block;
    font-size: 26px;
    padding: 15px;
}

.author-row-new sup {
    color: white;
    font-size: 60%;
}

.affiliations-new {
    font-size: 22px;
    text-align: center;
    width: 80%;
    margin: 0 auto;
    margin-bottom: 20px;
}

.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 26px;
}
.affiliatons {
    font-size: 22px;
}
.affil-row {
    margin-top: 18px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;
}
.inline-title h3 {
      display: inline;
    }
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0;
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    color: #666;
    margin-top: 4px;
    margin-bottom: 10px;
}
video {
    display: block;
    margin: auto;
}
figure {
    display: inline-block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
figure figcaption {
    text-align: center;
    font-size: 14px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    padding: 0;
    margin: 0;
    list-style: none;
}

.wrapper {
    display: flex;
}

.paper-btn-coming-soon {
    position: relative;
    top: 0;
    left: 0;
}

.coming-soon {
    position: absolute;
    top: -15px;
    right: -15px;
}

.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;

  background-color: #085ea8;
  color: white !important;
  font-size: 20px;
  width: 240px;
  font-weight: 600;
}
.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}

.paper-btn:hover {
    opacity: 0.85;
}

.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}

.venue {
    font-size: 30px;
}


/*.topnav {*/
/*    background-color: #EEEEEE;*/
/*    overflow: hidden;*/
/*}*/

/*.topnav div {*/
/*    max-width: 1500px;*/
/*    margin: 0 auto;*/
/*}*/

/*.topnav a {*/
/*    display: flex;*/
/*    color: black;*/
/*    text-align: center;*/
/*    !*vertical-align: middle;*!*/
/*    padding: 8px;*/
/*    text-decoration: none;*/
/*    font-size: 18px;*/
/*}*/

/*.topnav img {*/
/*    padding: 2px 0px;*/
/*    width: 30%;*/
/*    margin: 0.2em 0px 0.3em 0px;*/
/*    !*vertical-align: middle;*!*/
/*}*/

.image-container {
    display: flex;
    justify-content: flex-start;
    align-items: center;
    margin-top: 20px; /* Add some space at the top */
    padding-left: 20px;
}
.image-link img {
    width: 150px; /* You can adjust the width as needed */
    height: auto;
    margin: 0 10px; /* Adds horizontal spacing between images */
}
/*figure {*/
/*    display: block;        !* Ensures the figure is treated as a block-level element *!*/
/*    margin-left: auto;     !* Auto margin for horizontal centering *!*/
/*    margin-right: auto;    !* Auto margin for horizontal centering *!*/
/*    width: 80%;            !* Sets the width of the figure, matching the image width *!*/
/*}*/
/*img {*/
/*    width: 100%;           !* Makes the image expand to fill the figure *!*/
/*    display: block;        !* Removes any default margin/padding of the image *!*/
/*    margin-bottom: 0px;    !* Specific style requirement from your original HTML *!*/
/*}*/
pre {
    font-size: 0.9em;
    padding-left: 7px;
    padding-right: 7px;
    padding-top: 3px;
    padding-bottom: 3px;
    border-radius: 3px;
    background-color: rgb(235, 235, 235);
    overflow-x: auto;
}

.download-thumb {
    display: flex;
}

@media only screen and (max-width: 320px) {
    .download-thumb {
        display: none;
    }
}

.paper-stuff {
    width: 70%;
    font-size: 20px;
}

@media only screen and (max-width: 620px) {
    .paper-stuff {
        width: 100%;
    }
}

</style>

<script type="text/javascript" src="../js/hidebib.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
    <head>
        <title>One-step Diffusion Models with \(f\)-Divergence Distribution Matching</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:description" content="DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents"/>
        <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:creator" content="@xuyilun2">
        <meta name="twitter:title" content="DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents">
        <meta name="twitter:description" content="DisCo-Diff combines continuous diffusion models with learnable discrete latents, simplifying the ODE process and enhancing performance across various tasks.">
        <meta name="twitter:image" content="https://github.com/Newbeeer/disco-diff.github.io/blob/main/assets/dcdm_pipeline_fig_full.png">
    </head>

 <body>
<!--<div class="topnav" id="myTopnav">-->
<!--    <div>-->
<!--        <a href="https://www.nvidia.com/"><img src="assets/nvidia.svg"></a>-->
<!--        <a href="https://www.csail.mit.edu/" ><img src="assets/csail.png"></a>-->
<!--    </div>-->
<!--</div>-->
<!--    <div class="image-container">-->
<!--        &lt;!&ndash; Link to NVIDIA's website with their logo &ndash;&gt;-->
<!--        <a class="image-link" href="https://www.nvidia.com/">-->
<!--            <img src="assets/nvidia.svg" alt="NVIDIA Logo">-->
<!--        </a>-->
<!--&lt;!&ndash;        &lt;!&ndash; Link to MIT CSAIL's website with their logo &ndash;&gt;&ndash;&gt;-->
<!--&lt;!&ndash;        <a class="image-link" href="https://www.csail.mit.edu/">&ndash;&gt;-->
<!--&lt;!&ndash;            <img src="assets/csail.png" alt="CSAIL Logo">&ndash;&gt;-->
<!--&lt;!&ndash;        </a>&ndash;&gt;-->
<!--    </div>-->

<section class="jumbotron text-center" id="banner">
<div class="container">
    <div class="paper-title">
      <h1>One-step Diffusion Models with \(f\)-Divergence Distribution Matching</h1>
    </div>

    <div id="authors">
    	<center>
            <div class="author-row-new">
                <a href="https://yilun-xu.com/">Yilun Xu</a>
                <a href="https://weilinie.github.io/">Weili Nie</a>
                <a href="http://latentspace.cc/">Arash Vahdat</a>
            </div>
        </center>
        <center>

        <div class="row" style="margin-top:0rem; margin-left: auto; margin-right: auto;">
            <div class="col-md">
                <div id="affiliation"><img width="20%" src="assets/nvidialogo.png"></div>
            </div>
        </div>

<!--        <div class="affil-row">-->
<!--            <div class="venue text-center"><b>ICML 2024</b></div>-->
<!--        </div>-->

        </center>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="paper-btn" href="https://drive.google.com/file/d/14cQSW9oTIpBXM0_VqxbN-bGbBDcxka5T/view?usp=sharing">
                <span class="material-icons"> description </span>
                 Paper
            </a>
            <div class="paper-btn-coming-soon">
                <a class="paper-btn" href="">
                    <span class="material-icons"> code </span>
                    Code (Coming soon)
                </a>
            </div>
        </div></div>
    </div>
    </div>
    </section>
    <br>




        <center>
            <figure style="margin-top: 5px; margin-bottom: 10px;">
                <img width="80%" src="./assets/acceleration.png" style="margin-bottom: 15px;">
            </figure>
        </center>

        <center>
            <p style="margin-bottom: 5px;" class="caption">
                Inference latency of the teacher diffusion model and the \(f\)-distill. The FID scores are evaluated on ImageNet-64 (left) and MS-COCO (right)
            </p><p class="caption">
        </center>
        <center>
            <figure style="margin-top: 5px; margin-bottom: 5px;">
                <img width="90%" src="./assets/vis.png" style="margin-bottom: 5px;">
                <figcaption>Comparison between 50-step teacher (SD 1.5) and one-step student (\(f\)-distill) </figcaption>
            </figure>
        </center>

    <section id="abstract"/>
        <h2>Abstract</h2>
        <hr>
        <div class="flex-row">
            <p>
                Sampling from diffusion models involves a slow iterative process that hinders their practical deployment,
                especially for interactive applications.
                To accelerate generation speed, recent approaches distill a multi-step diffusion model into a single-step
                student generator via variational score distillation,
                which matches the distribution of samples generated by the student to the teacher's distribution.
                However, these approaches use the reverse Kullbackâ€“Leibler (KL)
                divergence for distribution matching which is known to be mode seeking. In this paper,
                <b>we generalize the distribution matching approach using a novel \(f\)-divergence
                minimization framework, termed \(f\)-distill,
                    that covers different divergences with different trade-offs in terms of mode coverage and training variance.</b>
                We derive the gradient of the $f$-divergence between the teacher and student distributions and show that it is
                expressed as the product of their score differences and a weighting function determined by their density ratio.
                This weighting function naturally emphasizes samples with higher density in the teacher distribution,
                when using a less mode-seeking divergence. We observe that the popular variational score distillation approach using the
                reverse-KL divergence is a special case within our framework.
                Empirically, we demonstrate that alternative \(f\)-divergences, such as forward-KL
                and Jensen-Shannon divergences, outperform the current best variational score distillation
                methods across image generation tasks. In particular, <b>when using Jensen-Shannon divergence, \(f\)-distill
                achieves current state-of-the-art
                one-step generation performance on ImageNet64 and zero-shot text-to-image generation on MS-COCO</b>.
            </p>
        </div>
    </section>


    <section id="intro"/>
        <h2>General \(f\)-divergence Minimization </h2>
        <hr>
        <div class="flex-row">
            <p> .
            </p>

            <center>
            <figure>
            <img width="70%" src="./assets/teaser.png" style="margin-bottom: 0px;">
            </figure>
            </center>

           <p style="margin-bottom: 5px;" class="caption">
                The gradient update for the one-step student in \(f\)-distill.
               The gradient is a product of the difference between the teacher score and fake score,
               and a weighting function determined by the chosen \(f\)-divergence and density ratio.
               The density ratio is readily available from the discriminator in the auxiliary GAN objective.
           </p><p class="caption">

            <p>
            </p>
            <p>
            </p>
        </div>


    </section>

    <section id="novelties"/>
        <h2>Behavior of different  \(f\)-divergences</h2>
        <hr>

        <center>
            <figure style="margin-top: 5px; margin-bottom: 5px;">
                <img width="90%" src="./assets/compare-f.png" style="margin-bottom: 5px;">
                <figcaption>Comparison of different \(f\)-divergences as a function of the likelihood ratio \(r :=p(\mathbf{x})/q(\mathbf{x})\) </figcaption>
            </figure>
        </center>

    </section>





    <section id="results">
        <h2>Experimental Results </h2>
        <hr>
        <div class="flex-row">
            <p><b>Image Synthesis</b>: We extensively validate \(f\)-distill on class-conditioned ImageNet (\(64 \times 64\) and text-to-image tasks (512ps).
                In practice, we use <a href="https://arxiv.org/abs/2206.00364">EDM</a> as teacher model on ImageNet-64, and <a href="https://arxiv.org/abs/2112.10752">Stable Diffusion v1.5</a> as teacher model on text-to-image tasks.
                We demonstrate that \(f\)-distill achieves the new state-of-the-art single-step performance on class-conditioned ImageNet-64 and zero-shot text-to-image generation on MS-COCO,
                <b>obtaining the FID score of 1.16 on ImageNet-64, and the FID score 7.42 on zero-shot MS-COCO.</b>
                For reference, the FID scores of teachers are 2.35 and 8.59 on the two datasets, respectively.
            </p>

            <center>
            <figure style="margin-top: 5px; margin-bottom: 5px;">
                <img width="100%" src="./assets/combine_exp.png" style="margin-bottom: 5px;">
            </figure>
            </center>

            <p style="margin-bottom: 5px;" class="caption">
               Quantitative comparison of \(f\)-distill with state-of-the-art
                multi-step and one-step generation methods, on ImageNet-64 (right) and MS-COCO (left)
            </p><p class="caption">



<!--            <p>In contrast to recent methods for accerlerated sampling of DDMs that abandon the ODE/SDE framework, GENIE can readily be combined with techniques such as classifier(-free) guidance (see examples below) and image encoding. These techniques can play an important role in synthesizing photorealistic images from DDMs, as well as for image editing tasks.-->
<!--            </p>-->
<!--            <center>-->
<!--                <div class="wrapper">-->
<!--                    <figure>-->
<!--                    <img width="70%" src="./assets/guidance.png" style="margin-bottom: 0px;">-->
<!--                    </figure>-->
<!--                </div>-->
<!--                </center>-->
<!--                <p class="caption">-->
<!--                    Image synthesis with classifier-free guidance for the ImageNet classes Pembroke Welsh Corgi and Streetcar using different numbers of denoising steps during generation.-->
<!--                </p><p class="caption">-->
<!--            <p>-->
<!--                We also train a high-resolution model on AFHQv2 (subset of cats only). We train a base DDM at resolution \(128 \times 128\) and a \(128 \times 128 \rightarrow 512 \times 512\) DDM-based upsampler. We are aiming to test whether GENIE also works for high-resolution image generation and in DDM-based upsamplers, which have become an important ingredient in modern large-scale DDM-based image generation systems.-->
<!--            </p>-->
<!--        </div>-->
<!--        <br>-->
<!--        <center>-->
<!--            <div class="wrapper">-->
<!--                <figure>-->
<!--                <img width="80%" src="./assets/cats_mixed_upsampled.png" style="margin-bottom: 10px;">-->
<!--                <img width="80%" src="./assets/cats_mixed_end_to_end.png" style="margin-bottom: 0px;">-->
<!--                </figure>-->
<!--            </div>-->
<!--            </center>-->
<!--            <p class="caption">-->
<!--                High-resolution images generated with the \(128 \times 128 \rightarrow 512 \times 512\) GENIE upsampler using only five function evaluations. For the two images at the top, the upsampler is-->
<!--                conditioned on test images from the Cats dataset. For the two images at the bottom, the upsampler is conditioned on samples from the \(128 \times 128\) GENIE base model (using 25 function evaluations);-->
<!--                an upsampler evaluation is roughly four times as expensive as a base model evaluation.-->
<!--            </p><p class="caption">-->
<!--        <br>-->
<!--        <figure>-->
<!--            <video class="centered" width="80%" autoplay loop muted playsinline class="video-background " >-->
<!--                <source src="assets/vid_twitter.mp4#t=0.001" type="video/mp4">-->
<!--                Your browser does not support the video tag.-->
<!--            </video>-->
<!--            <p class="caption">-->
<!--                The sequence above is generated by randomly traversing the latent space of our GENIE model (using 25 base model and five upsampler evaluations). We are interpolating in the latent space of the base model, and we keep the noise in the upsampler (both latent space and augmentation perturbations) fixed in all frames.-->
<!--            </p>-->
<!--        </figure>-->


    </section>

    <section id="paper">
        <h2>Paper</h2>
        <hr>
        <div class="flex-row">
            <div class="download-thumb">
            <div style="box-sizing: border-box; padding: 10px; margin: auto;">
                <a href="https://drive.google.com/file/d/14cQSW9oTIpBXM0_VqxbN-bGbBDcxka5T/view?usp=sharing"><img class="screenshot" src="assets/f-distill-preview.png"></a>
            </div>
        </div>
            <div class="paper-stuff">
                <p><b>One-step Diffusion Models with \(f\)-Divergence Distribution Matching</b></p>
                <p>Yilun Xu, Weili Nie, Arash Vahdat</p>
                <p><i></i></p>
                <div><span class="material-icons"> description </span><a href="https://drive.google.com/file/d/14cQSW9oTIpBXM0_VqxbN-bGbBDcxka5T/view?usp=sharing"> Paper</a></div>
                <div><span class="material-icons"> integration_instructions </span><a href=""> Code (coming soon)</a></div>
            </div>
            </div>
        </div>
    </section>

    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>@inproceedings{xu2024onestep,
    title={One-step Diffusion Models with \(f\)-Divergence Distribution Matching},
    author={Xu, Yilun and Nie, Weili and Vahdat, Arash},
    booktitle={},
    year={2024}
}</code></pre>
    </section>
</div>
</body>
</html>